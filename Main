from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from pandas import DataFrame as df
data=pd.read_csv('/content/train.tsv',sep="\t")
data.head(n=5)

data.columns = ['Text', 'Class']
data.groupby(['Class']).count()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data['Text'],data['Class'],test_size=0.3)
y_train

########## word2vec ###########
from gensim.utils import tokenize
questions = [list(tokenize(s, deacc=True, lower=True)) for s in X_train]

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
#from nltk.stem import PorterStemmer 
#porter = PorterStemmer()
for i in range(len(questions)):
    questions[i] = [w for w in questions[i] if w not in stopwords.words('english')] #Suppression des stop words
    questions[i] = [w for w in questions[i] if w not in '?,.;/:+=%ù£&\'`¨^*$€#@\"(§!)-_'] #Suppression des caractères spéciaux
questions[0]

from gensim.models import word2vec
model = word2vec.Word2Vec(questions, size=100, window=20,
                          min_count=2, workers=4, iter=100)
model.corpus_count
vocab = model.wv.vocab
list(vocab)[:10]
words = list((model.wv.vocab))
words[:40]
w1='marketing'
model.wv.most_similar(positive=w1)

model.save('trained_word2vec.bin')

import numpy

def get_vect(word, model):
    try:
        return model.wv[word]
    except KeyError:
        return numpy.zeros((model.vector_size,))

def sum_vectors(phrase, model):
    return sum(get_vect(w, model) for w in phrase)

def word2vec_features(X, model):
    feats = numpy.vstack([sum_vectors(p, model) for p in X])
    return feats

wv_train_feat = word2vec_features(X_train, model)
wv_train_feat.shape

#Classification using RandomForestClassifier:
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy import stats
from scipy.stats import randint
from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score
from sklearn.ensemble import RandomForestClassifier
clfwv = RandomForestClassifier()
wv_test_feat = word2vec_features(X_test, model)
clfwv.fit(wv_train_feat, y_train)
y_pred=clfwv.predict(wv_test_feat)
from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
#from sklearn.model_selection import cross_val_score
#scores = cross_val_score(clfwv, wv_test_feat, y_pred, cv=5)
#scores

random_grid = {'n_estimators': stats.randint(50, 100),
               'max_features': ['auto', 'sqrt'],
               'max_depth': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110],
               'min_samples_split': [2, 5, 10],
               'min_samples_leaf': [1, 2, 4],
               'bootstrap': [True, False]}
rf_random = RandomizedSearchCV(estimator = clfwv, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(wv_train_feat, y_train)
y_pred_rf_random=rf_random.predict(wv_test_feat)
rf_random.best_params_

print("Accuracy:",metrics.accuracy_score(y_test, y_pred_rf_random))

#Classification using xgboost:
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy import stats
from scipy.stats import randint
from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score
from xgboost import XGBClassifier
params = {'n_estimators': stats.randint(50, 100),
      'learning_rate': stats.uniform(0.01, 0.6),
      'subsample': stats.uniform(0.3, 0.9),
      'max_depth': [3, 4, 5, 6, 7, 8, 9],
      'colsample_bytree': stats.uniform(0.5, 0.9),
      'min_child_weight': [1, 2, 3, 4]
      }
xgb = XGBClassifier(objective='multi:softprob')
xgb.fit(wv_train_feat, y_train)
y_pred_xgb =xgb.predict(wv_test_feat)
#random_search.best_estimator_
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_xgb))
#scores = cross_val_score(xgb, wv_test_feat, y_pred_xgb, cv=5)
#scores

random_search = RandomizedSearchCV(xgb, param_distributions = params,n_iter = 10, scoring = 'accuracy', error_score = 0, verbose = 3, n_jobs = -1)
random_search.fit(wv_train_feat, y_train)
y_pred_random_search =random_search.predict(wv_test_feat)
random_search.best_estimator_
#scores2 = cross_val_score(random_search.best_estimator_, wv_test_feat, y_pred_random_search, cv=5)
#scores2

print("Accuracy:",metrics.accuracy_score(y_test, y_pred_random_search))

######### Neural nets #########
import numpy as np
from sklearn.neural_network import MLPClassifier
mlp= MLPClassifier(hidden_layer_sizes=(100, 100, 100,))
parameter_space = {
    'hidden_layer_sizes': np.arange(10, 15),
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam','lbfgs'],
    'alpha': [0.0001, 0.05,0.01],
    'learning_rate': ['constant','adaptive'],
}
from sklearn.model_selection import GridSearchCV

clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)
clf.fit(wv_train_feat, y_train)
# Best parameter set
print('Best parameters found:\n', clf.best_params_)

y_pred_mlp = clf.predict(wv_test_feat)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_mlp))

################## Word embedding using Spacy ##########
import spacy
# Load the spacy model that you have installed
nlp = spacy.load("en_core_web_sm")

import numpy

def spacy_sum_vectors(phrase, nlp):
    dec = nlp(phrase)
    return sum(w.vector for w in dec)

def spacy_word2vec_features(X, nlp):
    feats = numpy.vstack([spacy_sum_vectors(p, nlp) for p in X])
    return feats


wv_train_feat2 = spacy_word2vec_features(X_train, nlp)
print(wv_train_feat2.shape)

######### Random forest ########""
clfwv2 = RandomForestClassifier()
clfwv2.fit(wv_train_feat2, y_train)
wv_test_feat2 = spacy_word2vec_features(X_test, nlp)
y_pred2=clfwv2.predict(wv_test_feat2)
from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, y_pred2))

######### Random forest with Randomized search ########
random_grid = {'n_estimators': stats.randint(50, 150),
               'max_features': ['auto', 'sqrt'],
               'max_depth': [5, 8, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110],
               'min_samples_split': [2, 5, 10],
               'min_samples_leaf': [1, 2, 4],
               'bootstrap': [True, False],
               'criterion' :['gini', 'entropy']}
rf_random2 = RandomizedSearchCV(estimator = clfwv2, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random2.fit(wv_train_feat2, y_train)
y_pred_rf_random2=rf_random2.predict(wv_test_feat2)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_rf_random2))

#Xgboost:
xgb2 = XGBClassifier(objective="multi:softprob")
xgb2.fit(wv_train_feat2, y_train)
y_pred_xgb2 = xgb2.predict(wv_test_feat2)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_xgb2))

params = {'n_estimators': stats.randint(50, 150),
      'learning_rate': stats.uniform(0.01, 0.6),
      'subsample': stats.uniform(0.3, 0.9),
      'max_depth': [3, 4, 5, 6, 7, 8, 9],
      'colsample_bytree': stats.uniform(0.5, 0.9),
      'min_child_weight': [1, 2, 3, 4]
      }
random_search2 = RandomizedSearchCV(xgb2, param_distributions = params,n_iter = 10, scoring = 'accuracy', error_score = 0, verbose = 3, n_jobs = -1)
random_search2.fit(wv_train_feat2, y_train)
y_pred_random_search2 =random_search2.predict(wv_test_feat2)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_random_search2))
